{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df = pd.read_csv(\"Data/preprocessed_data_SVD.csv\",header=None)\n",
    "labels_df = pd.read_csv(\"Data/labels.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels_df.iloc[:,1]\n",
    "num_lab = int(np.max(np.unique(labels))) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize PCA reduced values\n",
    "svd_df=(svd_df-svd_df.mean())/svd_df.std()\n",
    "num_dim = svd_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels, num_classes = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split PCA data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    svd_df, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1942872, 25)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kayaz\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\kayaz\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(756, activation='relu', input_dim = num_dim))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(756, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_lab, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kayaz\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "1457154/1457154 [==============================] - 26s 18us/step - loss: 1.8617 - acc: 0.3632\n",
      "Epoch 2/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 1.4109 - acc: 0.5010\n",
      "Epoch 3/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 1.2260 - acc: 0.5572\n",
      "Epoch 4/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 1.1063 - acc: 0.5931\n",
      "Epoch 5/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 1.0201 - acc: 0.6191\n",
      "Epoch 6/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.9559 - acc: 0.6398\n",
      "Epoch 7/200\n",
      "1457154/1457154 [==============================] - 23s 15us/step - loss: 0.9035 - acc: 0.6559\n",
      "Epoch 8/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.8604 - acc: 0.6692\n",
      "Epoch 9/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.8236 - acc: 0.6809\n",
      "Epoch 10/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.7920 - acc: 0.6907\n",
      "Epoch 11/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.7636 - acc: 0.7001\n",
      "Epoch 12/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.7395 - acc: 0.7088\n",
      "Epoch 13/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.7173 - acc: 0.7160\n",
      "Epoch 14/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.6978 - acc: 0.7230\n",
      "Epoch 15/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.6789 - acc: 0.7293\n",
      "Epoch 16/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.6628 - acc: 0.7350\n",
      "Epoch 17/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.6480 - acc: 0.7404\n",
      "Epoch 18/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.6342 - acc: 0.7451\n",
      "Epoch 19/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.6211 - acc: 0.7495\n",
      "Epoch 20/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.6098 - acc: 0.7540\n",
      "Epoch 21/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5983 - acc: 0.7579\n",
      "Epoch 22/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5887 - acc: 0.7617\n",
      "Epoch 23/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5779 - acc: 0.7659\n",
      "Epoch 24/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5697 - acc: 0.7691\n",
      "Epoch 25/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5618 - acc: 0.7717\n",
      "Epoch 26/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5536 - acc: 0.7746\n",
      "Epoch 27/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5454 - acc: 0.7778\n",
      "Epoch 28/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5389 - acc: 0.7805\n",
      "Epoch 29/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5322 - acc: 0.7827\n",
      "Epoch 30/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5260 - acc: 0.7848\n",
      "Epoch 31/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5197 - acc: 0.7874\n",
      "Epoch 32/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5137 - acc: 0.7901\n",
      "Epoch 33/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5075 - acc: 0.7924\n",
      "Epoch 34/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.5020 - acc: 0.7945\n",
      "Epoch 35/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4980 - acc: 0.7962\n",
      "Epoch 36/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4929 - acc: 0.7980\n",
      "Epoch 37/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4882 - acc: 0.7996\n",
      "Epoch 38/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4833 - acc: 0.8017\n",
      "Epoch 39/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4793 - acc: 0.8033\n",
      "Epoch 40/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4748 - acc: 0.8047\n",
      "Epoch 41/200\n",
      "1457154/1457154 [==============================] - 24s 16us/step - loss: 0.4710 - acc: 0.8066\n",
      "Epoch 42/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4673 - acc: 0.8078\n",
      "Epoch 43/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4633 - acc: 0.8095\n",
      "Epoch 44/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4602 - acc: 0.8108\n",
      "Epoch 45/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4562 - acc: 0.8125\n",
      "Epoch 46/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.4532 - acc: 0.8134\n",
      "Epoch 47/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.4495 - acc: 0.8150\n",
      "Epoch 48/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.4463 - acc: 0.8158\n",
      "Epoch 49/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.4429 - acc: 0.8176\n",
      "Epoch 50/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.4402 - acc: 0.8185\n",
      "Epoch 51/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.4373 - acc: 0.8200\n",
      "Epoch 52/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4354 - acc: 0.8204\n",
      "Epoch 53/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4319 - acc: 0.8218\n",
      "Epoch 54/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4289 - acc: 0.8232\n",
      "Epoch 55/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4268 - acc: 0.8239\n",
      "Epoch 56/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4239 - acc: 0.8254\n",
      "Epoch 57/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4214 - acc: 0.8262\n",
      "Epoch 58/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4192 - acc: 0.8271\n",
      "Epoch 59/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4167 - acc: 0.8282\n",
      "Epoch 60/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4151 - acc: 0.8288\n",
      "Epoch 61/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4121 - acc: 0.8301\n",
      "Epoch 62/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4105 - acc: 0.8302\n",
      "Epoch 63/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4078 - acc: 0.8317\n",
      "Epoch 64/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4058 - acc: 0.8324\n",
      "Epoch 65/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.4037 - acc: 0.8333\n",
      "Epoch 66/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.4018 - acc: 0.8342\n",
      "Epoch 67/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.4000 - acc: 0.8347\n",
      "Epoch 68/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3985 - acc: 0.8354\n",
      "Epoch 69/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3966 - acc: 0.8360\n",
      "Epoch 70/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3949 - acc: 0.8369\n",
      "Epoch 71/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3922 - acc: 0.8378\n",
      "Epoch 72/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3903 - acc: 0.8386\n",
      "Epoch 73/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3889 - acc: 0.8393\n",
      "Epoch 74/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3872 - acc: 0.8401\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1457154/1457154 [==============================] - 23s 15us/step - loss: 0.3855 - acc: 0.8407\n",
      "Epoch 76/200\n",
      "1457154/1457154 [==============================] - 23s 15us/step - loss: 0.3843 - acc: 0.8408\n",
      "Epoch 77/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3826 - acc: 0.8419\n",
      "Epoch 78/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3815 - acc: 0.8424\n",
      "Epoch 79/200\n",
      "1457154/1457154 [==============================] - 25s 17us/step - loss: 0.3795 - acc: 0.8433\n",
      "Epoch 80/200\n",
      "1457154/1457154 [==============================] - 26s 18us/step - loss: 0.3781 - acc: 0.8435\n",
      "Epoch 81/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3765 - acc: 0.8444\n",
      "Epoch 82/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3752 - acc: 0.8448\n",
      "Epoch 83/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3735 - acc: 0.8455\n",
      "Epoch 84/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3717 - acc: 0.8461\n",
      "Epoch 85/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3713 - acc: 0.8462\n",
      "Epoch 86/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3698 - acc: 0.8469\n",
      "Epoch 87/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3674 - acc: 0.8481\n",
      "Epoch 88/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3668 - acc: 0.8481\n",
      "Epoch 89/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3658 - acc: 0.8486\n",
      "Epoch 90/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3651 - acc: 0.8489\n",
      "Epoch 91/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3626 - acc: 0.8501\n",
      "Epoch 92/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3620 - acc: 0.8502\n",
      "Epoch 93/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3607 - acc: 0.8507\n",
      "Epoch 94/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3592 - acc: 0.8513\n",
      "Epoch 95/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3585 - acc: 0.8523\n",
      "Epoch 96/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3569 - acc: 0.8522\n",
      "Epoch 97/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3563 - acc: 0.8528\n",
      "Epoch 98/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3545 - acc: 0.8534\n",
      "Epoch 99/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3537 - acc: 0.8536\n",
      "Epoch 100/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3528 - acc: 0.8543\n",
      "Epoch 101/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3517 - acc: 0.8544\n",
      "Epoch 102/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3505 - acc: 0.8552\n",
      "Epoch 103/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3496 - acc: 0.8553\n",
      "Epoch 104/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3485 - acc: 0.8558\n",
      "Epoch 105/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3482 - acc: 0.8559\n",
      "Epoch 106/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3469 - acc: 0.8567\n",
      "Epoch 107/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3456 - acc: 0.8572\n",
      "Epoch 108/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3449 - acc: 0.8572\n",
      "Epoch 109/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3434 - acc: 0.8580\n",
      "Epoch 110/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3425 - acc: 0.8584\n",
      "Epoch 111/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3420 - acc: 0.8586\n",
      "Epoch 112/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3414 - acc: 0.8586\n",
      "Epoch 113/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3397 - acc: 0.8594\n",
      "Epoch 114/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3391 - acc: 0.8598\n",
      "Epoch 115/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3385 - acc: 0.8602\n",
      "Epoch 116/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3371 - acc: 0.8606\n",
      "Epoch 117/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3364 - acc: 0.8613\n",
      "Epoch 118/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3359 - acc: 0.8612\n",
      "Epoch 119/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3348 - acc: 0.8615\n",
      "Epoch 120/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3348 - acc: 0.8614\n",
      "Epoch 121/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3333 - acc: 0.8618\n",
      "Epoch 122/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3322 - acc: 0.8627\n",
      "Epoch 123/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3322 - acc: 0.8625\n",
      "Epoch 124/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3312 - acc: 0.8631\n",
      "Epoch 125/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3304 - acc: 0.8635\n",
      "Epoch 126/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3297 - acc: 0.8637\n",
      "Epoch 127/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3288 - acc: 0.8643\n",
      "Epoch 128/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3283 - acc: 0.8644\n",
      "Epoch 129/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3275 - acc: 0.8648\n",
      "Epoch 130/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3266 - acc: 0.8650\n",
      "Epoch 131/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3258 - acc: 0.8654\n",
      "Epoch 132/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3251 - acc: 0.8660\n",
      "Epoch 133/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3245 - acc: 0.8658\n",
      "Epoch 134/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3237 - acc: 0.8662\n",
      "Epoch 135/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3232 - acc: 0.8663\n",
      "Epoch 136/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3224 - acc: 0.8669\n",
      "Epoch 137/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3213 - acc: 0.8672\n",
      "Epoch 138/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3209 - acc: 0.8674\n",
      "Epoch 139/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3200 - acc: 0.8678\n",
      "Epoch 140/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3197 - acc: 0.8680\n",
      "Epoch 141/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3190 - acc: 0.8683\n",
      "Epoch 142/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3186 - acc: 0.8685\n",
      "Epoch 143/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3182 - acc: 0.8685\n",
      "Epoch 144/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3174 - acc: 0.8685\n",
      "Epoch 145/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3166 - acc: 0.8692\n",
      "Epoch 146/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3154 - acc: 0.8694\n",
      "Epoch 147/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3153 - acc: 0.8698\n",
      "Epoch 148/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3147 - acc: 0.8701\n",
      "Epoch 149/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3142 - acc: 0.8699\n",
      "Epoch 150/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3136 - acc: 0.8707\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3121 - acc: 0.8710\n",
      "Epoch 152/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3123 - acc: 0.8714\n",
      "Epoch 153/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3120 - acc: 0.8711\n",
      "Epoch 154/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3115 - acc: 0.8711\n",
      "Epoch 155/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3104 - acc: 0.8720\n",
      "Epoch 156/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3097 - acc: 0.8721\n",
      "Epoch 157/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3095 - acc: 0.8722\n",
      "Epoch 158/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3090 - acc: 0.8724\n",
      "Epoch 159/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3083 - acc: 0.8727\n",
      "Epoch 160/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3074 - acc: 0.8735\n",
      "Epoch 161/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3079 - acc: 0.8726\n",
      "Epoch 162/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3073 - acc: 0.8731\n",
      "Epoch 163/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3071 - acc: 0.8734\n",
      "Epoch 164/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3053 - acc: 0.8742\n",
      "Epoch 165/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3057 - acc: 0.8740\n",
      "Epoch 166/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3049 - acc: 0.8743\n",
      "Epoch 167/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3045 - acc: 0.8746\n",
      "Epoch 168/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3040 - acc: 0.8744\n",
      "Epoch 169/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3033 - acc: 0.8748\n",
      "Epoch 170/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.3033 - acc: 0.8750\n",
      "Epoch 171/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3026 - acc: 0.8751\n",
      "Epoch 172/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3020 - acc: 0.8755\n",
      "Epoch 173/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3013 - acc: 0.8757\n",
      "Epoch 174/200\n",
      "1457154/1457154 [==============================] - 23s 15us/step - loss: 0.3011 - acc: 0.8759\n",
      "Epoch 175/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3004 - acc: 0.8760\n",
      "Epoch 176/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.3002 - acc: 0.8760\n",
      "Epoch 177/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2998 - acc: 0.8761\n",
      "Epoch 178/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.2990 - acc: 0.8767\n",
      "Epoch 179/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2988 - acc: 0.8770\n",
      "Epoch 180/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2983 - acc: 0.8770\n",
      "Epoch 181/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2978 - acc: 0.8772\n",
      "Epoch 182/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2978 - acc: 0.8773\n",
      "Epoch 183/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2964 - acc: 0.8778\n",
      "Epoch 184/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2966 - acc: 0.8775\n",
      "Epoch 185/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2959 - acc: 0.8778\n",
      "Epoch 186/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2957 - acc: 0.8781\n",
      "Epoch 187/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2950 - acc: 0.8785\n",
      "Epoch 188/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2946 - acc: 0.8788\n",
      "Epoch 189/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2948 - acc: 0.8782\n",
      "Epoch 190/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2941 - acc: 0.8789\n",
      "Epoch 191/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2935 - acc: 0.8793\n",
      "Epoch 192/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2925 - acc: 0.8794\n",
      "Epoch 193/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2921 - acc: 0.8797\n",
      "Epoch 194/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2914 - acc: 0.8799\n",
      "Epoch 195/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2918 - acc: 0.8798\n",
      "Epoch 196/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2913 - acc: 0.8799\n",
      "Epoch 197/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2906 - acc: 0.8804\n",
      "Epoch 198/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2909 - acc: 0.8804\n",
      "Epoch 199/200\n",
      "1457154/1457154 [==============================] - 23s 16us/step - loss: 0.2903 - acc: 0.8808\n",
      "Epoch 200/200\n",
      "1457154/1457154 [==============================] - 22s 15us/step - loss: 0.2901 - acc: 0.8806\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          epochs=200,\n",
    "          batch_size= 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485718/485718 [==============================] - 4s 9us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "import pickle\n",
    "pickle.dump(model, open(\"cnn_model_200ep_756n_svd.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.933183452130224"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
